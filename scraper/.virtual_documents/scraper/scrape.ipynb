from bs4 import BeautifulSoup
import requests
import pandas as pd





list_url='https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Computer%2BScience%2B&location=Ireland&geoId=104738515&trk=public_jobs_jobs-search-bar_search-submit&start=25'

response=requests.get(list_url) #Get request to url
list_data=response.text 
list_soup=BeautifulSoup(list_data,"html.parser") #parse html
page_jobs=list_soup.find_all('li')
#print(page_jobs)
jobs_available=[]


jobs_fetched=0
total_jobs=25
while jobs_fetched<total_jobs:
    id_list=[]
    for job in page_jobs:
        base_card_div = job.find("div",class_="base-card") #list of jobs
        job_id=base_card_div.get("data-entity-urn").split(":")[3] #accesses third item in dict which is id
        #print(job_id)
        id_list.append(job_id) #extracts job ids and saves them
       # print(id_list)
        
    
    for job_id in id_list:
        list_url=f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}' #Loops through individual job pages by id
        r=requests.get(list_url)
        list_data2=r.text
        job_post={} #job post dictionary to save to csv
        job_info=BeautifulSoup(list_data2,"html.parser") #parse html of individual jobs
        
        comp_name=job_info.find('a',class_='topcard__org-name-link topcard__flavor--black-link')
        
        comp = comp_name.text.strip() if comp_name else "Company name not found" #if not found wont produce error
        job_post["Job_name"] = comp
        applicant_info = job_info.find('div', class_='top-card-layout__entity-info-container')
    
        num_applicants_element =job_info.find('span', class_='num-applicants__caption')
        num_applicants = num_applicants_element.text.strip() if num_applicants_element else "Number of applicants not found"
        job_post["Number of applicants"] = num_applicants 
    
        time_element=job_info.find('span',class_="posted-time-ago__text")
        time=time_element.text.strip() if time_element else "Date posted not found"
        job_post["Date posted"] = time
        
        location_element = job_info.find('span', class_='topcard__flavor topcard__flavor--bullet')
        loc = location_element.text.strip() if location_element else "Location not found"
        job_post["Location"] = loc
        
        role_element=job_info.find('h2',class_='top-card-layout__title')
        role=role_element.text.strip() if role_element else "Role not found"
        job_post["Job title"] = role
        
        print(f"Company name: {comp}")
        print(f"Role: {role}")
        print(f"Location: {loc}")
        print(f"Posted {time} ago")
        print(f"Number of applicants: {num_applicants}")
        print("        ")
        jobs_available.append(job_post) 
        jobs_fetched+=1
    start=len(id_list) #updates start number of get request based on how many post have been scraped. Starts on next post to go onto next page
#print(jobs_available)


jobs_df = pd.DataFrame(jobs_available)
jobs_df


jobs_df.to_csv('job-list.csv',index=False)


















